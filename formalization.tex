% !TEX root = ./main.tex
\section{\gcoql}\label{sec:form}

\fo{Review this after all} In this section we describe our formalization of \gql in \coq. We start by defining a schema and its properties, then the graph data model and finally we review queries and their semantics. The definitions are as close as possible with respect to the \spec. This eyeball correspondence between the definitions in prose and the code gives a first level of trust that our formalization is correct, following the examples of X, Y and Z. Whenever there is a mismatch we point it out and explain the reasoning behind each decision.


\subsection{Schema}\label{subsec:schema}
We formalize schemas and type definitions following the \spec. A schema is represented as a record, containing a list of type definitions and a root type that specifies the available queries (\eg \texttt{Query} in the example from Figure \ref{fig:schema_ex}):
%

\begin{minted}[bgcolor=coqbg]{coq}
Record graphQLSchema := GraphQLSchema {
    query_type : Name;
    type_definitions : seq TypeDefinition }.
\end{minted}
%
Schemas may also include additional root types for specifying mutations and subscriptions\footnote{https://graphql.github.io/graphql-spec/June2018/\#sec-Schema}. These operations are, however, out of the scope of our formalization.

Our formalization of type definitions closely follows the \spec, as depicted in Figure~ \ref{fig:types_def}. A type may be a scalar type, an object type, which possibly implements a set of interfaces, an interface type, a union type or an enumeration type. Object and interface type definitions comprise an (ordered) set of fields; union types are defined by a set of type names and enumeration types by a set of values. For the corresponding type definitions to be valid, all such three sets should not be empty. While the \spec enforces this requirement syntactically (see the grammar on the left of Figure~ \ref{fig:types_def}), we perform this validation when verifying the (overall) well-formedness schemas.     
%
\setlength{\grammarparsep}{10pt plus 1pt minus 1pt} % increase separation between rules
\begin{figure*}[h]
    \centering
    \begin{subfigure}{.5\textwidth}
    \begin{grammar}
    <TypeDefinition> ::= \textbf{\texttt{scalar}} <name>
%    \alt \textbf{\texttt{type}} <name> \textbf{\texttt{\{}} <Field>$^{+}$ \textbf{\texttt{\}}}
    \alt \textbf{\texttt{type}} <name> [\textbf{\texttt{implements}} <name>$^{+}$] \textbf{\texttt{\{}} <Field>$^{+}$ \textbf{\texttt{\}}}
    \alt \textbf{\texttt{interface}} <name> \textbf{\texttt{\{}} <Field>$^{+}$ \textbf{\texttt{\}}}
    \alt \textbf{\texttt{union}} <name> \textbf{\texttt{=}} <name> (\textbf{\texttt{|}} <name>)*
    \alt \textbf{\texttt{enum}} <name>  \textbf{\texttt{\{}}  <name>$^{+}$ \textbf{\texttt{\}}}

    <Field> ::= <name> \textbf{\texttt{(}} <Arg>* \textbf{\texttt{) :}} <type>

    <Arg> ::= <name> \textbf{\texttt{:}} <type>

    <type> ::= <name>
    \alt \textbf{\texttt{[}}  <type> \textbf{\texttt{]}}
    \end{grammar}

    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
    \begin{minted}[bgcolor=coqbg]{coq}
    Inductive TypeDefinition : Type :=
    | ScalarTypeDefinition (name : Name)
    | ObjectTypeDefinition (name : Name)
                           (interfaces : seq Name)
                           (fields : seq FieldDefinition)
    | InterfaceTypeDefinition (name : Name)
                              (fields : seq FieldDefinition)
    | UnionTypeDefinition (name : Name)
                          (members : seq Name)
    | EnumTypeDefinition (name : Name)
                         (members : seq EnumValue).

    Inductive type : Type :=
    | NamedType : Name -> type
    | ListType : type -> type.
    \end{minted}

    \end{subfigure}
    \caption{Definition of \gql types, the \spec's grammar (left) and \coq implementation (right). Square brackets represent optional values. \fo{In the grammar, I would push the scalar case to a new line, to match the \coq snippet on the right}}
    \label{fig:types_def}
\end{figure*}


% As can be seen from the figure, our implementation looses information about non-emptiness of fields, union and enum members. We push this validation to a posterior predicate, as well as the discussion about the reasons behind this decision, to the following paragraphs.

% As can be seen in the figure, we tried to match the \spec's definition as much as possible. This eyeball correspondence gives us a degree of confidence about the implementation.  % We currently do not include the \textit{Input Object} types, as well as anything related to \textit{introspection}.

On this regard, observe that the provided definitions of schema and types enable building ``invalid'' schemas. For instance, one can build an object that implements scalar types or use a nonexistent type as the query type. To avoid this problem, the \spec provides several validation rules, scattered throughout the document\footnote{Most can be found in the \textbf{Type Validation} subsection of each type described in https://graphql.github.io/graphql-spec/draft/\#sec-Type-System.}.  We collect these rules and refer to them as the \textit{well-formedness} condition of a \gql schema.


\begin{definition}
A \gql schema is \textit{well-formed} if: 
\begin{itemize}
    \item its root type (is defined and) is an object type, 
    \item there are no duplicated type names, and
    \item every type definition is \textit{well-formed}.
\end{itemize}
\end{definition}

In our \coq formalization, this is captured by the Boolean predicate below. As mentioned in the introduction, our formalization heavily relies on Boolean reflection, following the SSReflect mindset.
%
\begin{minted}[bgcolor=coqbg]{coq}
Definition is_a_wf_schema (s : graphQLSchema) : bool :=
      is_object_type s s.(query_type) &&
      uniq s.(schema_names) &&
      all is_wf_type_def s.(type_definitions).
\end{minted}
%
The notion of well-formedness for type definitions requires \eg that union members contain existent object types and that object and interface types contain at least one field. Due to space limitations, we omit the full definition and refer the interested reader to file \texttt{SchemaWellFormedness.v}.

%We will, though, resume the discussion about non-emptiness of fields, union and enum members, which are included in the predicate. The main reason behind this decision is that, even though the \spec embeds this information in the grammar, it still includes it in their validation rules later on. We believe that it is simpler to use common lists instead of defining new structures or using dependent types, from an implementation point of view, while still preserving the correspondence to the algorithmic description given by the \spec.
%\td{Not sure if correctly worded... but it was just simpler to use lists. A non-empty list structure required coercions to lists and then redefining some lemmas and things. Or using dependent types (sigma type) adds complexity when proving and defining things (at least that was the case for me)}


% There are two main reasons why we push this rule to a separate predicate instead of embedding it in the structure itself. The first one is that, even though the \spec embeds it in the grammar, it still includes it in a validation rule later on. To match their definition and preserve the eyeball correspondence, we also include it. The second reason is that we use SSReflect and it is simpler to use \mintinline{coq}{seq} directly and all its theory, instead of defining coercions and repeating definitions for a new structure.

For convenience, we encapsulate schemas with their well-formedness proof in a single structure. This structure also contains predicate \mintinline{coq}{is_a_valid_value}, which determines whether a value used in a query indeed matches the scalar type expected by the schema. For example, if a field argument has type \texttt{Float}, the \spec requires that the actual argument used in query represent a double-precision fractional value.\footnote{The \spec declares a set of minimal scalar values and how they should be represented, such as floating-point values adhering to IEEE 754. We do not include this base restrictions but leave it open to implementation.}
%
\begin{minted}[bgcolor=coqbg]{coq}
Record wfGraphQLSchema := WFGraphQLSchema {
    schema : graphQLSchema;
    _ : schema.(is_a_wf_schema);
    is_a_valid_value : type -> Vals -> bool; }.
\end{minted}

% This predicate is necessary to establish when a value used in a query or in the graph actually matches the scalar type expected by the schema. For instance, if an argument requires a \texttt{Float} value, then the actual value passed to the query must be something that represents a double-precision fractional value\footnote{The \spec declares a set of minimal scalar values and how they should be represented, such as floating-point values adhering to IEEE 754. We do not include this base restrictions but leave it open to implementation.}. This predicate validates that this is satisfied.

% Due to space constraints, we omit the definition of \textit{well-formedness} for type definitions. This property includes things such as: interfaces and objects must declare at least one field, objects correctly implement their declared interfaces, union types are not empty and contain only object types, amongst others. These definitions are collected from the \spec \td{Scattered throughout the \spec*}.

Having reviewed or formalization of schemas, we now discuss our formalization of the data model. 


\iffalse
\begin{minted}{coq}
Let Animal := Interface "Animal" {[::
                (Schema.Field "name" [::] "String");
                (Schema.Field "friends" [::] ["Animal"])
            ]}.
Let Dog := Object "Dog" implements [:: "Animal"] {[::
            (Schema.Field "name" [::] "String");
            (Schema.Field "friends" [::] ["Animal"]);
            (Schema.Field "favouriteToy" [::] "Toy")
        ]}.
\end{minted}
\fi


\subsection{Data Model}\label{subsec:graph}
Following \HP, for our formalization we adopt a data model based on graph, where data are modeled as directed property graphs, with labeled edges and typed nodes. Nodes contain a type and a set of properties (key-value pairs) and  edges contain single labels that describe the relation between nodes. Also, every property and label may contain a list of arguments (key-value pairs).

To represent the values associated to properties or arguments, we consider the type \Vals. 
% A value in \Vals may be a single scalar value or a list of values\td{This choice is based on the limitations of the model}.\fo{I think we should address this in the Discussion section, where we discuss the limitations of our formalization. We could even remove the last sentence from the paragraph.}

\begin{definition}
A \emph{\gql graph} over \Vals{} is defined by the following elements:
\begin{itemize}
    \item A root node.
    \item A collection of edges of the form ($u$, \texttt{f[}$\alpha$\texttt{]}, $v$), where $u, v$ are nodes and \texttt{f[}$\alpha$\texttt{]} is a label with arguments (key-value pairs).
\end{itemize}
\end{definition}

In our formalization, this is captured by the following structures, where \texttt{label} represents labels over an edge or keys in a node's property. 
%\td{Probably the name is not the best}\fo{Fully agree. I suggest changing \texttt{fld} to \texttt{label}, \texttt{Field} to \texttt{Label} and \texttt{label} to \texttt{name} (or \texttt{lname} if there is a clash) }. 
%
\begin{minted}[bgcolor=coqbg]{coq}
Record label := 
  Label { lname : string; args : seq (string * Vals) }.

Record node := 
  Node { ntype : Name; nprops : seq (label * Vals) }.

Record graphQLGraph := 
  GraphQLGraph { root : node; E : seq (node * label * node) }.
\end{minted}
%
\fo{Tomás, why did you represent graphs as a seq---rather than as a set--of edges?} The root node of a \gql graph represents the starting point from where queries are evaluated.

Intuitively, the data modeled by a \gql graph is expected to follow (the type system described by) a schema. However, the definition of graphs is fully independent schemas. To relate the data to the type system, we define the notion of \textit{conformance} of a graph \wrt a schema.
%
\begin{definition}
A \gql graph $\mathcal{G}$ \textit{conforms} to a schema $\mathcal{S}$ if:
\begin{itemize}
    \item the types of $\mathcal{G}$ root node and $\mathcal{S}$ query root coincide, 
    \item every edge of $\mathcal{G}$ \textit{conforms} to $\mathcal{S}$, and
    \item every node of $\mathcal{G}$ \textit{conforms} to $\mathcal{S}$.
\end{itemize}
\end{definition}
%
The conformance of nodes includes rules such as checking that a node type is declared as an object type in the schema and that its properties are defined as fields in the corresponding type. Among others, the conformance of edges ensures that edge labels are declared as a field in the source node's type and the target node has a type compatible with the field's return type. Full definitions of these two notions are found in file \texttt{GraphConformance.v}.

With this in mind, the notion of conformance of a graph \wrt a schema is formalized as follows:
%
\begin{minted}[bgcolor=coqbg]{coq}
Definition is_a_conforming_graph 
      (s : wfGraphQLSchema) (g : graphQLGraph) : bool :=
  root_type_conforms s g.(root) &&
  edges_conform s g &&
  nodes_conform s g.(nodes).
\end{minted}
%
As a side note, this notion was only loosely defined in \HP, but it is properly addressed in a recent work of one of its authors ~\cite{olafschema}.

Similarly to \gql schemas, we define a structure that encapsulates the notion of a \textit{conformed} graph, containing a graph and a proof of its conformance to a particular schema.

\begin{minted}[bgcolor=coqbg]{coq}
Record conformedGraph (s : wfGraphQLSchema) :=
  ConformedGraph { graph : graphQLGraph;
                       _ : is_a_conforming_graph s graph }.
\end{minted}

 %These properties include validation rules such as: every node must have an object type and their properties must be defined in their associated type, or an edge's label must be declared as a field in the source node's type and the target node must have a type compatible to the field's return type, among other things.

%With both the schema and the underlying data model we can proceed to define \gql queries and their semantics.


\subsection{Queries}\label{subsec:query}
To define queries we faithfully follow the \spec, as shown in Figure~\ref{fig:query_def}. A query consists of an optional name and a list of selections. A selection can be a single field, a field followed by a set of subselections or an inline fragment comprising a type condition and a set of subselections. Fields can be accompanied by a list of arguments and can also be renamed.

\begin{figure*}[h]
  \centering
  \begin{subfigure}{.5\textwidth}
    \begin{grammar}
    		<Query> ::= [\textbf{\texttt{query}} [<name>]] \textbf{\texttt{\{}} <Selection>$^{+}$ \textbf{\texttt{\}}}
		
        <Selection> ::= <name> \textbf{\texttt{(}} <Arg>* \textbf{\texttt{)}}
        \alt <alias> \textbf{\texttt{:}} <name> [\textbf{\texttt{(}} <Arg>$^{+}$ \textbf{\texttt{)}}]
        \alt <name> [\textbf{\texttt{(}} <Arg>$^{+}$ \textbf{\texttt{)}}] \textbf{\texttt{\{}} <Selection>$^{+}$ \textbf{\texttt{\}}}
        \alt <alias> \textbf{\texttt{:}} <name> [\textbf{\texttt{(}} <Arg>$^{+}$ \textbf{\texttt{)}}] \textbf{\texttt{\{}} <Selection>$^{+}$ \textbf{\texttt{\}}}
        \alt \textbf{\texttt{... on}} <name> \textbf{\texttt{\{}} <Selection>$^{+}$ \textbf{\texttt{\}}}
        
        <Arg> ::= <name> \textbf{\texttt{:}} <value>
    \end{grammar}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}

    \begin{minted}[bgcolor=coqbg]{coq}
    Record query := 
         Query { qname : option string; 
                 selection_set : seq Selection }.
                   
    Inductive Selection : Type :=
    | SingleField (name : Name)
                  (arguments : seq (Name * Vals))
    | AliasedField (alias : Name)
                   (name : Name)
                   (arguments : seq (Name * Vals))
    | NestedField (name : Name)
                  (arguments : seq (Name * Vals))
                  (subqueries : seq Query)
    | NestedAliasedField (alias : Name)
                         (name : Name)
                         (arguments : seq (Name * Vals))
                         (subqueries : seq Selection)
    | InlineFragment (type_condition : Name)
                     (subqueries : seq Selection).
    \end{minted}
  \end{subfigure}
  \caption{Definition of \gql queries, the \spec's grammar (left) and the \coq implementation (right).}\td{To be completely accurate, the parenthesis in field selections are not necessary unless there are arguments}\td{This might add some noise?}
  \label{fig:query_def}
\end{figure*}

Intuitively, a valid query has a tree structure, where leaves correspond to fields of scalar types and inner nodes correspond either to fields of some object type or abstract type (interface or union)\fo{abstract type not defined}\td{Ok like this?}, or to inline fragments that condition when their subselections are evaluated. 

\iffalse
%For instance, the query in Figure \ref{fig:qres_ex} can be depicted as the tree in Figure \ref{fig:query_tree}.
\begin{figure}
    \centering
    \includegraphics[scale=0.33]{imgs/query_tree.png}
    \caption{\gql query as a tree.}
    \label{fig:query_tree}
\end{figure}
\fi 

% Similar to the schema definition, we follow the \spec's grammar as closely as possible, as can be seen from Figure~\ref{fig:query_def}. We do not embed the property of non-emptiness of subqueries in the definition because the \spec pushes it to a different validation rule, even though it is already embedded in the grammar, and we believe it is simpler to reuse the \texttt{seq} library and its existent functionalities. 

Observe that the definition of queries in Figure~\ref{fig:query_def} is not bound to any schema, requiring thus a separate validation process to ensure that they respect or adhere to a given schema. We introduce the notion of query \textit{conformance}, based on a set of validation rules scattered throughout the \spec (\cf\S5~\cite{gqlspec}). The validity of queries depends on the validity of their selection sets, which in turn requires the notion of \textit{type in context} at a given selection location.\footnote{The \spec refers to it as type in scope or parent type. Some algorithmic descriptions make use of the types in context but are not explicit in their signatures.} % Since queries are selections over the fields of types, it is important to know exactly to what type they are being applied.
To illustrate this, consider the following query with two occurrences of field \texttt{name}.

%\begin{minipage}[t]{.25\textwidth}
\begin{minted}[escapeinside=||, mathescape=true]{js}
query {
  goodboi {
    |$\ldots$| on Dog {
      name
      favoriteToy
    }
    |$\ldots$| on Pig {
      name
      favoriteToy
    } } }
\end{minted}
%\end{minipage}%
% \begin{minipage}[t]{.25\textwidth}
% \begin{minted}[escapeinside=||, mathescape=true]{js}   
% type Human {
%   age: Int
% }

% type Martian {
%   age: Float
% }
% \end{minted}
% \end{minipage}
%\et{for space reason, we may want to take some liberties wrt the formatting of queries and responses (see some examples on this page)}


\noindent In one case, the field is requesting information about the \texttt{Dog} type, while in the other it is requesting information about case about the \texttt{Pig} type. (These are the respective types in context for the first and second occurrence of the field.) The distinction is important because some field selections might be valid in some contexts but not in others. This is the case, for instance, for field \texttt{favoriteToy}: It is valid in the context of a \texttt{Dog} type but it is invalid in the context of a \texttt{Pig} type, as \texttt{Pig} type does not contain any such field. Similarly, the above query would also be invalid if \texttt{Pig} type contained a so-called field, but of type different from \texttt{Toy} ---the type the same field in type \texttt{Dog}.

%Just as in the case of well-formedness of schemas or conformance of graphs, queries must go through a validation process. We define the property of \textit{conformance} of queries, based on validation rules scattered throughout the \textit{Validation} section of the \spec\footnote{https://graphql.github.io/graphql-spec/June2018/\#sec-Validation}.

% Before describing the validation process, it is very important to address the notion of \textit{type in context} where queries are defined and used. The type in context is the type over which a user might be requesting information on its fields. To illustrate this, consider the following query. The first selection, namely \texttt{goodboi}, is requesting information about the query type, meaning that it is used in the context of the \texttt{Query} type. Moving onto the  \texttt{name} subselection, it is not direct which is. In one case, the type in context is \texttt{Dog}, while in the other the field is used in the context of the \texttt{Pig} type.

% The importance of this type in context is that fields or inline fragments might be valid in certain cases but not in others. Similarly, a field may have a particular return type in one case and a different one in another type, like in the following example. Both types have an \texttt{age} field, but in one case it returns an integer value while in the other a floating point value. If that field is encountered in a query, it is necessary to know to which type it is being requested.

Now that we have clarified the notion of type in context, we are in condition of defining the notion of query conformance. 

%\td{A query conforms when its selections conform to the Query type}
\begin{definition}
A \gql selection set $ss$ \textit{conforms} to a schema \schema over a type in context $ts$ if:
\begin{itemize}
    \item every selection in $ss$ is consistent, 

    \item field merging between field selections is possible, and
    % During the evaluation process, fields with the same response name are collected and merged to ensure that they are all executed at the same time. This validation rule checks that it makes sense to merge those fields. The following example illustrates two queries that have the same response name but should not be merged. The first one is accessing the field \texttt{name} while the second is accessing the field \texttt{age} but renaming it to \texttt{name}. Both are selections on different fields of the same type but with the same response name.
    % \begin{minted}{graphql}
    %                query {
    %                    name
    %                    name:age
    %                }
    %\end{minted}

    \item fields with same response name have compatible response shapes.\fo{We should be more precise about what we mean by ``shape''}
    % This checks whether two fields with the same response name will produce response values that are consistent to each other. These values should be unambiguous for a user. For instance, the following example\td{These examples look a bit off I think.} shows two queries that produce similar responses but with ambiguous values. In the first one, we ask for dog's \texttt{name}s, which are strings, and in the second for pig's \texttt{age}s, which are integers. We also rename the \texttt{age} value to \texttt{name}. The responses we get will have some cases where \texttt{name} is associated to a string and other where it is associated to integers.
    % \begin{minted}[escapeinside=||,mathescape=true]{graphql}
    %                query {
    %                    |$\ldots$| on Dog {
    %                        name
    %                    }
    %                    |$\ldots$| on Pig {
    %                        name:age
    %                    }
    %                }
    % \end{minted}
\end{itemize}
\end{definition}

\begin{definition}
A \gql query $\varphi$ \textit{conforms} to a schema \schema if its selection set conforms to \schema over the query root type.
\end{definition}

%\et{this is the beginning of the problem I'm having with your statements about queries. There is the notion of *a query*, noted $\varphi$, yet from now on your statements are about *queries*. The definition below mixes "the content" (consistency, compatible response, etc.) with "the sequence" (all, list comprehension, etc). Worse, later on in the development, you use $\varphi$ as the variable name of type "seq Queries" (while fig6 uses a bar over). See also my later comments about your "plural formulations"}

In our formalization, this is captured as follows:

%The implementation can be seen in the code below, however due to space limitations we only informally describe the %three mentioned rules. The complete definitions can be found in the file \texttt{QueryConformance.v}.

\begin{minted}[bgcolor=coqbg, escapeinside=@@, mathescape=true]{coq}
Definition query_conforms 
    (s : wfGraphQLSchema) (@$\varphi$@ : query) : bool :=
    selections_conform s s.(query_type) @$\varphi$@.(selection_set).
    
Definition selections_conform (s : wfGraphQLSchema)
    (ts : Name) (selections : seq Selection) : bool :=
        all (is_consistent ts) selections &&
        is_field_merging_possible ts selections &&
        have_compatible_response_shapes
            [seq (ts, s) | s <- selections].

\end{minted}
%\fo{This introduces the conformance of a query \wrt a schema, but the schema is never mentioned in the definition. It needs to be fixed!!!}

The first rule specifies when a selection holds by itself. For instance, if the selection is a field, the rule checks that the field is part of the type in context and that its arguments are correctly provided; if the selection is an inline fragment, then the type condition has to be valid with respect to the type in context \fo{when can it not be valid?}\td{If both the type condition and the type in context do not share at least one subtype (subtype is reflexive). Not sure if this is relevant here though}.%The \spec defines this rule in several different sections.

The second rule validates when fields can be correctly merged during evaluation, which is an essential aspect of the semantics, since it ensures that repeated fields are only evaluated once. To illustrate this, consider the following query, where a user requests information on the fields \texttt{name} and \texttt{oink} of pigs. The user decides to rename the field \texttt{oink} to \texttt{name}, however this is invalid, since both fields cannot be merged and evaluated once; both fields refer to distinct pieces of information in the system.

\begin{minted}[escapeinside=||,mathescape=true]{js}
query {
  goodboi {
    |$\ldots$| on Pig {
	  name
	  name:oink
} } }
\end{minted}

The last rule is necessary to check when fields generate consistent values when evaluated. For instance, in the query below a user might request the names of dogs and the oinkness of pigs, but renaming the latter to have the same response name as the former. This query is considered invalid because evaluating it might produce a response that contains entries where the key \texttt{name} is associated to both string values and floating point numbers. 

\begin{minipage}[t]{.22\textwidth}
\begin{minted}[escapeinside=||,mathescape=true]{js}
query {
  goodboi {
    friends {
      |$\ldots$| on Dog {
        name
      }
      |$\ldots$| on Pig {
        name:oink
} } } }
\end{minted}
\end{minipage}%
\begin{minipage}[t]{.22\textwidth}
\begin{minted}[escapeinside=||,mathescape=true]{js}
// Possible invalid output
{
  "goodboi" : {
    "friends" : [
        { "name" : "Marle" },
        { "name" : 9000 }
    ]
}  }
\end{minted}
\end{minipage}



				
These last two rules, for merging and unambiguous values, are defined as a single validation rule in the \spec\footnote{https://graphql.github.io/graphql-spec/June2018/\#sec-Field-Selection-Merging}, however we decide to separate them. The first reason, which we cover more extensively in \S\ref{sec:discussion}, is that it is possible to optimize the algorithm by reducing redundant recursive calls. Secondly, splitting facilitates reasoning about the predicates.

% The main issue is that the \spec allows for what we call \textit{invalid fragments}, originally described in an issue in the \spec's repository\footnote{https://github.com/graphql/graphql-spec/issues/367}. In a nutshell, the \spec allows using fragments with type conditions that can span to multiple unrelated types. These end up not being evaluated due to posterior checks\footnote{https://graphql.github.io/graphql-spec/June2018/\#DoesFragmentTypeApply()}.


This concludes the definition of \gql queries and the validation process. \td{Examples can be found in the files \texttt{SpecExamples.v} and \texttt{HPExample.v}}. For the rest of the paper, it is assumed that queries conform to a given schema. 

\subsection{Semantics}\label{subsec:semantics}
Now we have all the prerequisites to define the semantics of \gql queries and their selection sets. We begin by briefly examining the responses generated by executing queries and then we give an informal description of the semantics, finishing with the formal definition. %We finish by discussing some implementation choices and comparison with the \spec and \HP.

A first observation we need to make is that query semantics is not compositional, in the sense that the result of a sequence of selections is not obtained by concatenating the results of the individual selections. Therefore, our semantic function will take a (whole) sequence of selections and return a (whole) sequence of responses.  

We model responses with a tree structure, similar to JSON, as shown in Figure~\ref{fig:response_def}. A response can be a value, an object mapping keys to other responses or an array of response values.

\begin{figure*}[h]
\centering
\begin{subfigure}{.5\textwidth}
    \begin{grammar}
    		<ResponseValue> ::= <value>
		\alt \textbf{\texttt{\{}} (<name> \textbf{\texttt{:}} <ResponseValue>)* \textbf{\texttt{\}}}
		\alt  \textbf{\texttt{[}} <ResponseValue>* \textbf{\texttt{]}}
 \end{grammar}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
\begin{minted}[bgcolor=coqbg]{coq}
Inductive ResponseValue (A : Type) : Type :=
| Leaf : A -> ResponseValue
| Object : seq (Name * ResponseValue) -> ResponseValue
| Array : seq ResponseValue -> ResponseValue.

Definition GraphQLResponse (Vals: eqType) :=
    seq (Name * (@ResponseNode (option Vals))).
\end{minted}
   
  \end{subfigure}
  \caption{Definition of \gql responses, the grammar matching JSON (left) and the \coq implementation (right).}
  \label{fig:response_def}

\end{figure*}
%

%\fo{What is the role of \texttt{ArrayResponse}? When presenting our domain of values \textit{Vals} you mentioned that it can be a scalar value, or a ``list'' of values. Is the \texttt{ArrayResponse} the counterpart of this ``list''? }

%\fo{Explain the role of each constructor}
%\fo{Shouldn't the ObjectResponse constructor take a GraphQLResponse argument?}

Our model of a query response is based in that it allows us to preserve similarity to selections and provides a strong reasoning principle, as well as facilitating the preservation of order of the individual response values.  This differs from the \spec, which only states that responses are a map from keys to values\td{unspecified} and does not impose an ordering, although encourages it (\cf\S7.2.2~\cite{gqlspec}). We believe that preserving the order is one of the strong selling points for \gql (selections and their responses are very similar and easy to read). 

%\begin{figure}[h]

%\caption{Implementation in Coq of \gql responses.}
%\label{fig:responses}
%\end{figure}

\td{Move to discussion?} Our approach has two main disadvantages: possible duplication of response names and cost of access. Since we use lists instead of maps, we can encounter duplicated names and accessing a value has a linear cost given by the lists size, instead of the constant access obtainable with a map. We still argue that the reasoning principles and simplicity to order is highly valuable. Nevertheless, we include a proof that the results obtained with the semantics have no duplicated names. Finally, we use option types to represent null values in the leaves of the response tree.



As we described in \S\ref{subsec:graph}, the underlying data model we use is a graph, therefore the semantics are instantiated to this setting. %In a following paragraph we briefly explore an alternative that is closer to the \spec, in the sense that it can be detached from a particular data model. 
Informally, the evaluation of the selection set of a query represents a navigation over a graph, starting from the root node, traversing its edges and collecting data from its nodes. In this sense:
\begin{itemize}
    \item A field selection represents either accessing a node's property or traversing an edge to a neighboring node.
    \item An inline fragment conditions whether using a node to access its properties or to traverse to other nodes.
    \item Subselections are evaluations over neighboring nodes.
\end{itemize}

\begin{definition}
Let $\mathcal{G}$ be a graph and $ss$ a selection set, both conforming to a schema $\mathcal{S}$. The evaluation of $ss$ over $\mathcal{G}$ from node $u \in \mathit{nodes}(\mathcal{G})$, denoted $\llbracket ss \rrbracket^{u}_{\mathcal{G}}$, is defined recursively as shown in Figure~\ref{fig:semantics}. 
\end{definition}

The definition of the evaluation function for selections, displayed in Figure~\ref{fig:semantics}, only refers to the base case of an empty list, field selections without renaming and inline fragments. The complete definition can be found in the file \texttt{QuerySemantics.v}.
\et{you need to explain the figure - guide the reader to understand it.}

\begin{definition}
Let $\mathcal{G}$ be a graph and $\varphi$ a query containing a selection set $ss$, both conforming to a schema $\mathcal{S}$. The semantics of $\varphi$ over $\mathcal{G}$ is defined as the result of evaluating $ss$ from the root node of $\mathcal{G}$.
\end{definition}

The semantics of queries is then formalized as follows:
\begin{minted}[bgcolor=coqbg, escapeinside=||, mathescape=true]{coq}
Definition execute_query (s : wfGraphQLSchema)
    (g : conformedGraph s) (|$\varphi$| : query) :=
    execute_selection_set s g.(root) |$\varphi$|.(selection_set).
\end{minted}

%The formal definition of the semantics is depicted in Figure \ref{fig:semantics}. The definition displays the cases where a field selection is accessing a node's property, when it is navigating to other nodes and when it is evaluating an inline fragment. Aliased fields are omitted for brevity but the complete definition can be explored in the file \texttt{QuerySemantics.v}.


\td{Move to discussion?}
The main difference with respect to \HP and the main similarity to the \spec is that we perform a collection of fields at the query level, whereas \HP performs a post-processing of responses. The main reasons are our approach is more similar to the \spec and that it is harder to reason using \HP's approach.

%\td{Don't know how to present this or if it is relevant but I include it so you know it exists - The spec first groups fields by name and then evaluates each group. These are different steps. We define everything in one place because it was easier to reason about the semantics. The difficulty with the spec's approach is that when proving things about the semantics, you have to provide, at each step, the information on how your data is structured (every group contains only fields and every field in a group has the same response name as the group's key used to group them). This is problematic mostly because of inline fragments, which do not have response names. Definitions must always account for the case of inline fragments, even when they are thought out to be used only on fields. Our definition is maybe less compositional but thanks to Equations, it is simpler to reason about it - because Equations generate a nice reasoning principle. We also attempted defining it similarly to the spec, still preserving the information on the structure by using dependent types and Equation, but the resulting reasoning principle would end up being huge (80-100 cases) (I believe I read Matthieu saying that nesting some definitions could exponentially increase the size of the reasoning principle, which is what would occur in this case).}

\et{you might want to say something in the discussion section about this choice, yes. But quick and to the point. I.e. why following spec instead of HP (query vs. response level), and why departing from spec in some aspect. The discussion about the impact on reasoning in Coq is *super relevant* for the CPP crowd!}

%Our first attempt at defining the semantics was to follow \HP's post-processing approach. Our intention was to be as close as possible to their formalization to later prove their transformation and equivalence results, which we cover in Section~\ref{sec:norm}. However, the non-structural recursive nature of both the transformations and the post-processing function made reasoning about semantic equivalence very hard.

\begin{figure*}[t]
\small
    \centering
    \begin{align*}
    % Empty
    & (1) & \eval{\cdot}{u} &= [\cdot] \\
    % SingleField
    & (2) & \evalu{\fld\; ::\; \queries} &= \begin{cases}
        \resp{\texttt{coerce(\val)}} \; ::\; \evalfilteru{\queries}{\fkey}  
        & \text{if } \mathit{u.property}(\fld) = \val \\
        \resp{\nval} \; :: \; \evalfilteru{\queries}{\fkey} 
        & \text{otherwise}
    \end{cases}\\
    % Nested field
    & (3) & \evalu{\nfld{\overline{\beta}} \; ::\; \queries} &=
    \begin{cases}
        \resp{\texttt{[} \mathit{map}\; (\lambda v_{i}.\; \eval{\overline{\beta} \mdoubleplus \mathit{merge (collect_\fkey (\queries))}}{v_{i}})\; \mathit{neighbors(u)} \texttt{]}} \; :: \; \evalfilteru{\queries}{\fkey} \\  
        \hfill \text{ if } 
            \mathit{type}_{\mathit{u.type}}(\fkey) \text{ is } \; L_{t} \text{ and } \{v_{1}, \ldots, v_{k}\} = 
        \{v_{i} \mid (u, \fkey[\alpha], v_{i}) \in E\} \\
        %
    (\fkey:\{\eval{\subqueries{\beta}}{v}\})\; :: \; \evalfilteru{\queries}{\fkey}  
        \hfill \text{ if } 
        \mathit{type(\fkey)} \text{ is not } L_{t} \text{ and } (u, \fkey[\alpha], v) \in E \\
    %
    \resp{\nval}\; :: \; \evalfilteru{\queries}{\fkey} 
    \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad
    \text{ if } \mathit{type(\fkey)} \text{ is not } L_{t}  \text{ and } \nexists v \text{ s.t. }  (u, \fkey[\alpha], v) \in E \\
    \end{cases}\\
    %inline fragment
    & (4) & \evalu{\ifrag{t}{\overline{\beta}}\; ::\; \queries} &= \begin{cases}
    \evalu{\overline{\beta} \mdoubleplus \queries} & \mathit{fragment\_type\_applies}_{\texttt{u.type}}(t)\\
    \evalu{\queries} & \sim
    \end{cases}
    \end{align*}
    \caption{Semantics of \gql queries. The $\mathit{property}$ function is an accessor to a node's property value. The $\mathit{filter}$ function removes all fields with the given response name, while $\mathit{collect}$ gathers all fields that match the given response name. 
    Meanwhile, $\mathit{merge}$ joins the subqueries of the input selections. The $\mathit{type}$ function retrieves the type of the given field in the node's type.
	The $\mathit{fragment\_type\_applies}$ predicate checks if the fragment's type condition is related to the node's type and whether its contents should be evaluated in the given node.
	Finally, $E$ are the graph's edges, while $L_{t}$ is simply the list type.
  %  \fo{This definition is given using syntax you have never introduced (what does :: mean? what does f:v mean? what does ++ mean?, etc, etc). Also, you must explain what all auxiliary functions (filter, property, merge, neighbors, etc, etc) do} 
   % \et{explain also what are $u$ $G$ $L_t$ and $E$}
%    \et{rename does\_fragment\_type\_apply to fragment\_type\_applies (and make it hide the $=$ true.)}
    }
    \label{fig:semantics}
\end{figure*}

%The first one is that we currently do not handle errors during execution. This is due to two main reasons: the evaluation function assumes it receives valid queries and we have not yet implemented non-null types. These relates to the two kinds of errors one may encounter when evaluating \gql queries: validation and execution errors. The first ones are captured before execution and displayed to the user. Our semantics has to deal with a case which would be ruled out by the validation process. We believe both cases can be covered by including X (monad/reasonably exceptional type theory/etc)\td{rewrite}.

% The second major aspect refers to completeness. Both our formalization and \HP's do not generate all possible results expected by a \gql service. In particular, there is a limitation when generating lists with a nesting bigger than one. it does not generate results for list types of depth bigger than one, when its inner type is not a scalar type\footnote{HP goes a step further and does not allow any type of nested list result.}. For instance, one might want to get information about friends but grouped by their age. This could be modeled as a field with type \texttt{[[Human]]}, where the list type has depth 2. A response for this query would look something like \texttt{"friends":[[...], ..., [...]]}. This response cannot be generated by our semantics\footnote{It can be defined with the \mintinline{coq}{Response} structure but not generated with the semantics.}.

%The main challenge in this case is to define what this nested list types represent in a graph. If we take a simple case of a field with type \texttt{[Human]}, we can model it as neighbors of a node. However, if we increase the nesting such as \texttt{[[Human]]}, it becomes harder to model. What does this represent in the graph? Should we introduce blank nodes in between the source node and the \texttt{Human} nodes? Are these inner edges labeled? Should there be a blank node per each level of nesting or a single one with edges to itself? All these questions do not have a straightforward answer. Our semantics, as the one definded in PH, simply ignores any nesting bigger than one.\td{This is where it can be modelled using Functors. The \spec checks if it received a collection and applies map to eventually get to the concrete values. Not sure how to put this out there.}

This concludes the base formalization of \gql schemas, graph data model, and queries and their semantics.  These definitions provide the base upon which further development and analysis can be developed.

%\td{I feel there is not much to say about the semantics... The "juicy" bits are in the discussion section - limitations given by the graph model, functor approach to the spec's semantics, etc.}

\subsection{Design considerations and discussion}\label{subsec:discussion}
Intro to section

%\et{I suggest to split this section in two parts: one that is related to the core (schema, data model, queries, semantics), and on that is related to normalization/simplified semantics. And to move the first to just after section 3 (either as a 3.5 or as a separate section 4 if it's length justifies it)}

\subsubsection*{\HP Schema}

%\et{I don't get what is the point of this paragraph, and why it matters -- what's the takeaway message? how does it relate to what we did?}
%We believe that the \spec's definition of the schema can be slightly confusing and ambiguous. The schema is described as being ``\textit{defined in terms of the types and directives it supports as well as the root operation types for each kind of operation}''\footnote{https://graphql.github.io/graphql-spec/June2018/\#sec-Schema}. However, the \spec also defines a structure called \texttt{schema}\footnote{https://graphql.github.io/graphql-spec/June2018/\#SchemaDefinition} that only contains the root operation types (query, mutation and subscription), meanwhile the type definitions and directives are defined separately. We think this introduces the first ambiguity to the definition. Now, to actually capture the notion described above, it is necessary to define what is called a \textit{Document}\footnote{https://graphql.github.io/graphql-spec/June2018/\#Document}, consisting of a list of definitions and queries. Among these definitions one can include the \texttt{schema} structure, as well as the type definitions and directives (by repeatedly using the \textit{Type System Definition}\footnote{https://graphql.github.io/graphql-spec/June2018/\#TypeSystemDefinition} rule). This effectively permits building the schema as expected, however we think it is unintuitive to allow mixing definitions with queries. Also, the naming of the different structures seems to introduce confusion and inconsistency.

 
% the \spec defines each of the three elements (types, directives and root operation types) as separate entities. The first candidate is the \textit{Type System} \footnote{https://graphql.github.io/graphql-spec/June2018/\#TypeSystemDefinition} but it is a disjunction of the three elements: types, directives and root operation types. 


% Secondly, the \spec defines the \texttt{schema}\footnote{https://graphql.github.io/graphql-spec/June2018/#SchemaDefinition} structure that only contains the root operation types (query, mutation and subscription). The type definitions and directives are defined separately. Even though it is not a major difference, it lends itself to possible confusion when referring to the schema of a \gql service. In addition, the previously quoted definition does


%The previously quoted definition actually matches the \textit{Type System} structure\footnote{https://graphql.github.io/graphql-spec/June2018/\#TypeSystemDefinition}. Our formalization follows the latter but rename it to schema to also match the quoted description.

%\et{contrast with what you did, and the tradeoffs (what's convenient, what's not)}
The definition of \gql schemas in \HP is primarily done by defining sets over the possible field names, types and values, as well as a collection of functions to relate the different elements.
While this is convenient in their setting because it facilitates their reasoning with graphs and simplifies many validation rules, we believe it is not the case for our formalization.
First and mostly because it is too different from how the \spec is defined\footnote{It is worth mentioning that when \HP was published, the \spec did not provide the schema definition DSL, among other things.} and 
we regard similarity as a component of trust. Secondly, because we expect our implementation to be extracted in the future, the definitions should be as natural as possible to what
a programmer would use, which we believe is achieved in our work and would otherwise not have been by following \HP's  approach.

Finally,  the definition of consistency of a schema given by \HP does not capture all of the validation rules established by the \spec. 
In particular, it does not properly account for arguments of implemented fields in objects implementing interfaces. 
A corrected version is provided in a recent work by Hartig\&Hidder~\cite{olafschema}.

\subsubsection*{Data model and semantics}

In this section, we address a limitation on the graph data model that is used in our formalization, as well as in \HP and~\cite{olafschema}.
We first describe the limitation, then its consequences on the semantics of queries, and finally some thoughts on what causes this issue and
its potential impact.

The limitation we discovered with the graph model is that it does not entirely describe the universe of a \gql service. 
The graphs cannot model list types of nesting depth greater than one, when the wrapped type is an object or abstract type. 
\HP takes this a step further, also not being able to model nested list types for scalar types.\td{neither HP nor the other work mention this}

The main consequence is that the semantics is not capable of producing every expected outcome of a query. To illustrate this, 
consider a selection over a field whose type is \texttt{[[Animal]]}. 
The expected \gql output when evaluating this field is a list of list of objects.
However, the corresponding graph for this scenario cannot be described with our model, hence the semantics simply ignores the 
nesting and generates a list of objects for the given field.

The main issue is that it is not entirely clear for us what these nested list types represent in a graph or how they should be modeled. 
Should there be blank nodes in between the source node and the neighboring nodes with the actual data? Is each edge in the path labeled?
What occurs with each increment in the nesting depth? Since the information is ultimately collected from ``concrete'' nodes, should the graph be kept the same 
but introduce \textit{formatter} functions to modify the responses, such that they match the expected values? How does this differ from the \gql resolvers?

Finally, we are not entirely certain about the impact that this limitation may have on realistic \gql services. 
Empirical studies, such as~\cite{empiricalgql} and~\cite{empiricalapi}, analyze the structure of \gql schemas 
over a collection of industrial and open-source projects. Their results provide some insights such as the most 
common object types, which can be traced to reference implementations and shown to not have nested list types 
with depth greater than one. However, these insights are not conclusive, thus requiring a more profound analysis.\td{hint to future work? In particular~\cite{empiricalgql} seems more complete and with more tooling} 

Regardless of this open problem, we believe that it introduces complexity to the formalization but the results and those of \HP should still be preserved.
\td{Why?  Idk, but the essence of the issue doesn't seem that terrible in that the results change or the semantics goes wild and generate 
unexpected results. It should only reflect in changes in how you navigate the graph but not in the actual data one collects. The content one is looking for
is still the same.}


%As described in Section~\ref{subsec:graph}, \gql is agnostic to the technology used and the underlying data model. We follow \HP and instantiate the semantics to a graph setting, allowing to reason about it\et{see my comment in the intro}. We describe here severe limitations \et{why is the list nesting situation "severe"?? what backs up this characterization?} that this data model introduces on the possible results generated and the open questions regarding how to properly model certain aspects of \gql schemas \et{vague}. This model is exploited by \HP and \cite{olafschema} but neither discuss nor mention the limitations. \et{you only discuss one, without really explaining why it matters, or to what extent. Does it break all established results? etc.}

%The main issue with this graph model is that there is no proper accounting of list types containing other list types (with any nesting depth). When it comes to list types it is not clear what they represent in a graph. Let us illustrate this with an example.
%The different features that compose a \gql schema can be represented in a graph somehow. For instance, a field is either a property or the label of an edge, while its return type can be associated to a target node in an edge. However, when it comes to list types it is not clear what they represent in a graph. Let us illustrate this with an example.


% Our definition is in essence the same as in \HP but differs greatly in implementation. \HP defines a \gql graph in a more ``centralized'' manner. For instance, nodes and field names are defined by sets. Node types are defined by a single function which receives a node identifier and gives its type. Properties are also defined by a single function which receives a node identifier and a field name with arguments. Contrarily, our approach attempts to recreate the structures individually. For instance, a node contains all the information pertaining to itself; its type and its properties. We believe this is a more natural approach to defining the graph from an engineering point of view.

%A service may declare the field \texttt{friends:[Human]} in a given type, representing the list of friends.
%In a graph this can be pictured as having a node with multiple outgoing edges labeled \texttt{friends}, reaching other nodes of type \texttt{Human}. It is possible to then extend the service by including a new field \texttt{friendsByName:[[Human]]}, in which one can request a list of friends but grouped by their names. At the moment neither our implementation, \HP nor \cite{olafschema} properly handle this situation. The open question is what does this represent in the graph? These should be outgoing edges similarly to the previous case but, what should the target nodes be? Should these be intermediate blank nodes? Is every edge labeled or only the last one that reaches a node with type \texttt{Human}? What happens if we increase the nesting? Since the information is ultimately collected from ``concrete'' nodes, should the graph be kept the same but introduce \textit{formatter} functions to match the schema? How does this differ from the \gql resolvers?

%These questions and more \et{what are the more? instead of spending half a page on nested lists, tell us what are the other limitations and what their possible impact is -- ie. like a typical "threats to validity" section} are not addressed nor discussed in \HP and it is actually more restrictive than expected\et{expected by whom?}, by not allowing nested lists for scalar values (in nodes's properties)\et{in the previous paragraph, you put all three approaches in the same bag, now it seems \HP does worse}. Meanwhile, our approach and the one used in \cite{olafschema} allow any list type at the property level but simply ignore any possible nesting when the list type refers to neighboring nodes (composite types), as in the example above.\et{for instance, a typical question would be: "how prevalent are those situations?" if you don't know the answer to this question, how can you evaluate if it's severe or anecdotical?}
%In the case of \cite{olafschema}, they do not address nor discuss these questions\et{you said that already}. This choice of modeling has some consequences\et{what are they?} when defining the semantics of \gql queries, because the possible results generated are restricted to a smaller subset\et{the limitation seemed to be about the data model, not the queries or their results}. It is not clear what the proper way is to handle this issue\et{still talking about nested lists?} but more is explored in Section~\ref{subsec:semantics}\et{watch out: 3.4 is now *before* this text}. 
%We also address the \spec's semantics and how this is managed.\et{?}

\subsubsection*{Queries}

\et{overall verbose and not very clear/insightful}

%\HP differs from both the \spec and \gcoql when defining queries, as they include an additional rule for lists of queries. Their grammar includes a production rule for lists of queries, which is defined at the same level of \et{same....as...} the other rules. The main issue we found with this approach is that it allows building arbitrary trees instead of just a list of queries.\et{you say on page 5 that "a valid query has a tree structure"!} These trees can be flattened to recover the list structure but increasing the effort when defining functions and reasoning over queries. We believe this is assumed by \HP but not explicitly mentioned otherwise.


On a different note, we mentioned in Section~\ref{subsec:query} that we split a validation rule\et{which?} into two separate predicates\et{which?}. The reason behind this is that we noticed that the \spec's definition includes redundant recursive calls\et{meaning?} which may result in increased computational time\et{are you worrying about performance in the model?}. By splitting the definition in two parts, we expect to optimize the algorithm and also facilitate reasoning about them\et{why?}. At the time of writing this paper, a new algorithm was proposed by a team at XING\footnote{https://www.xing.com/} that also addresses this very same issue and is described in~\cite{xingalg}. They follow a different approach to resolving it, using sets, and provide a much more elaborate analysis of execution times than us\et{sure, you don't provide any ;-)}. Comparing both approaches and analyzing execution times could be an interesting venue to explore.

Finally, we also noticed that the previous rule\et{which?} is too conservative and may consider valid queries as invalid. This occurs because the \spec allows defining fragments that are never evaluated. The issue is that the validation rule can then consider the subqueries in these fragments as invalid, even though they are never evaluated, rendering the whole query invalid\footnote{An example query can be seen in the following link: https://tinyurl.com/y3hz5vgv.}. We attempt to remove this conservativeness in the predicate that checks the merger of fields but we have not provided proofs that it does. For the predicate regarding unambiguous results, it is equally conservative as the \spec. We believe it should not be hard to modify the definition, however we decided against it at the moment, to keep it simple and facilitate reasoning over the predicate, as well as preserving some similarity to the \spec\td{Although it may not be entirely similar from the start...}.


\subsubsection*{Semantics}

%We finish this section by addressing two major aspects about our formalization; completeness and errors.

The first one\et{?} was briefly mentioned in Section~\ref{subsec:graph}\et{seems this has been moved}, when discussing the limitations and open questions regarding the graph model. These translate in the fact that we currently do not produce list results with nested lists of objects. For instance, the field \texttt{friendsByName:[[Human]]} is treated as if it were defined as \texttt{friendsByName:[Human]} and the results match the latter format. Otherwise, there is no restriction in the case of nested lists for scalar values\et{yep, that was in 5.2}. In \HP, there is no possibility to produce nested lists for either scalar or object values\footnote{The grammar itself does not permit it.} and there is no mention of this restriction.\et{all this was said before}

Regarding error handling, we currently do not implement it. Errors may have two main sources; validation errors and execution errors.\td{Not sure how to write this}\et{does not belong here---more of a "limitations" section (before the conclusion)}

\et{so this section seems vacuous}

%\et{overall, after looking at 5.1-5.4, I think that a compressed/selected list of points good be put in a "3.6 Design considerations and discussion" section}

